"use strict";(self.webpackChunksign_detection=self.webpackChunksign_detection||[]).push([[10],{7010:(l,s,u)=>{u.r(s),u.d(s,{AboutModule:()=>h});var c=u(9808),r=u(6210),t=u(1223);const a=[{path:"",component:(()=>{class o{constructor(){}ngOnInit(){}}return o.\u0275fac=function(e){return new(e||o)},o.\u0275cmp=t.Xpm({type:o,selectors:[["app-about"]],decls:19,vars:0,consts:[["href","https://angular.io"],["href","https://www.tensorflow.org/js"],["href","https://github.com/tensorflow/tfjs-models/tree/master/handpose"],["href","https://github.com/andypotato/fingerpose"]],template:function(e,d){1&e&&(t.TgZ(0,"h1"),t._uU(1,"About the Demo"),t.qZA(),t.TgZ(2,"section")(3,"p"),t._uU(4," This demo is developed using "),t.TgZ(5,"a",0),t._uU(6,"Angular"),t.qZA(),t._uU(7," and "),t.TgZ(8,"a",1),t._uU(9,"TensorFlow.js"),t.qZA(),t._uU(10,". "),t.qZA(),t.TgZ(11,"p"),t._uU(12," The project is using the "),t.TgZ(13,"a",2),t._uU(14,"handpose"),t.qZA(),t._uU(15," model to recognize hands and their features. After that, we pass the output to "),t.TgZ(16,"a",3),t._uU(17,"fingerpose"),t.qZA(),t._uU(18," which estimates the hand sign. Once we recognize an action, we emit it via a BehaviorSubject. A component, subscribed to the BehaviorSubject processes the user action. "),t.qZA()())},styles:["a[_ngcontent-%COMP%]{color:#81b7ff;text-decoration:none}a[_ngcontent-%COMP%]:hover{color:#fff;text-decoration:underline}section[_ngcontent-%COMP%]{max-width:400px;display:block}"]}),o})()}];let i=(()=>{class o{}return o.\u0275fac=function(e){return new(e||o)},o.\u0275mod=t.oAB({type:o}),o.\u0275inj=t.cJS({imports:[r.Bz.forChild(a),r.Bz]}),o})(),h=(()=>{class o{}return o.\u0275fac=function(e){return new(e||o)},o.\u0275mod=t.oAB({type:o}),o.\u0275inj=t.cJS({imports:[c.ez,i]}),o})()}}]);